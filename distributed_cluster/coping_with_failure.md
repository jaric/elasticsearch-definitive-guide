# 故障恢復

前文我們已經提到過 Elasticsearch 可以應對節點故障。讓我們來嘗試一下。如果我們把第一個節點殺掉，我們的集羣就會如下圖所示：

![殺掉一個節點後的集羣](../images/02-06_node_failure.png)

被殺掉的節點是主節點。而爲了集羣的正常工作必須需要一個主節點，所以首先進行的進程就是從各節點中選擇了一個新的主節點：`Node 2`。

主分片 `1` 和 `2` 在我們殺掉 `Node 1` 後就丟失了，我們的索引在丟失主節點的時候是不能正常工作的。如果我們在這個時候檢查集羣健康狀態，將會顯示 `red`：存在不可用的主節點！

幸運的是，丟失的兩個主分片的完整拷貝在存在於其他的節點上，所以新的主節點所完成的第一件事情就是將這些在 `Node 2` 和 `Node 3` 上的從分片提升爲主分片，然後集羣的健康狀態就變回至 `yellow`。這個提升的進程是瞬間完成了，就好像按了一下開關。

那麼爲什麼集羣健康狀態依然是是 `yellow` 而不是 `green` 呢？是因爲現在我們有3個主分片，但是我們之前設定了1個主分片有2個從分片，但是現在卻只有1份從分片，所以狀態無法變爲 `green`，不過我們可以不用太擔心這裏：當我們再次殺掉 `Node 2` 的時候，我們的程序**依舊**可以在沒有丟失任何數據的情況下運行，因爲 `Node 3` 中依舊擁有每個分片的備份。

如果我們重啓 `Node 1`，集羣就能夠重新分配丟失的從分片，這樣結果就會與**三節點兩從集羣**一致。如果 `Node 1` 依舊還有舊節點的內容，系統會嘗試重新利用他們，並只會複製在故障期間的變更數據。

到目前爲止，我們已經清晰地瞭解了 Elasticsearch 的橫向擴展以及數據安全的相關內容。接下來，我們將要繼續討論分片的生命週期等更多細節。
